{
  "nodes": [
    {
      "id": "alignment.pretraining",
      "type": "module",
      "doc": ""
    },
    {
      "id": "alignment.pretraining._assert_finite",
      "type": "function",
      "doc": ""
    },
    {
      "id": "alignment.pretraining._check_grad_finite",
      "type": "function",
      "doc": ""
    },
    {
      "id": "alignment.pretraining.main",
      "type": "function",
      "doc": "Train a small HTRNet on the given image list using dictionary configuration."
    },
    {
      "id": "alignment.losses",
      "type": "module",
      "doc": ""
    },
    {
      "id": "alignment.losses.ProjectionLoss",
      "type": "class",
      "doc": "Entropic\u2011regularised optimal\u2011transport projection loss with optional\nunbalanced formulation.\n\nParameters\n----------\nreg : float, optional\n    Entropic regularisation strength passed to ``ot.sinkhorn2`` or\n    ``ot.unbalanced.sinkhorn_unbalanced2``.  Default = 0.1.\nunbalanced : bool, optional\n    If ``True`` use the unbalanced OT formulation\n    ``ot.unbalanced.sinkhorn_unbalanced2`` (requires an additional\n    ``reg_m`` parameter that can be passed through ``sinkhorn_kwargs``);\n    otherwise use the balanced formulation ``ot.sinkhorn2``.\n    Default = ``False`` (balanced).\nsupervised_weight : float, optional\n    Scale for the supervised descriptor distance term.  Default = 1.0.\nsinkhorn_kwargs : dict, optional\n    Extra keyword arguments forwarded to the solver (e.g. max_iter, tol,\n    log, reg_m for the unbalanced case).\n\nForward Inputs\n--------------\ndescriptors : torch.Tensor                # shape (N, d)\n    Source features extracted by the network.\nword_embeddings : torch.Tensor            # shape (M, d)\n    2\u2011D \u201cvocabulary\u201d embedding coordinates (target support).\naligned : torch.Tensor                    # shape (N,)\n    For each descriptor, the index of its aligned word in\n    ``word_embeddings`` (\u20121 if no alignment available).\ntgt_probs : torch.Tensor                  # shape (M,)\n    Marginal probabilities for the target distribution.  For the balanced\n    version it will be renormalised to sum to 1.\n\nReturns\n-------\nloss : torch.Tensor\n    Scalar loss = OT loss + supervised alignment loss."
    },
    {
      "id": "alignment.losses.SoftContrastiveLoss",
      "type": "class",
      "doc": "InfoNCE\u2011style loss that pulls together image descriptors whose\ntranscripts have small Levenshtein distance.\n\nParameters\n----------\ntau   : float  \u2013 temperature in image space (distance \u2192 similarity).\nT_txt : float  \u2013 temperature in transcript space (controls softness).\neps   : float  \u2013 numeric stability."
    },
    {
      "id": "alignment.losses._ctc_loss_fn",
      "type": "function",
      "doc": "A thin wrapper around `torch.nn.functional.ctc_loss` that takes *logits*."
    },
    {
      "id": "alignment.plot",
      "type": "module",
      "doc": "Plotting utilities for alignment module."
    },
    {
      "id": "alignment.plot.plot_dataset_augmentations",
      "type": "function",
      "doc": "Save a figure showing three images and their augmentations side by side.\n\nParameters\n----------\ndataset : HTRDataset\n    Dataset providing images and augmentation transforms.\nsave_path : str\n    Where to write the PNG figure."
    },
    {
      "id": "alignment.plot.plot_tsne_embeddings",
      "type": "function",
      "doc": "Generate a coloured t-SNE plot of backbone embeddings and save it.\n\nFeatures and current alignment labels are harvested from ``dataset`` using\n``backbone``. t-SNE then projects the descriptors to 2\u2011D and the scatter\nplot colours samples in blue when ``aligned == 1`` and black otherwise.\n\nParameters\n----------\ndataset : HTRDataset\n    Dataset instance providing the images.\nbackbone : HTRNet\n    The visual backbone model to extract embeddings from.\nsave_path : str\n    Path where the generated t-SNE plot (PNG image) will be saved."
    },
    {
      "id": "alignment.plot.plot_projector_tsne",
      "type": "function",
      "doc": "Plot t-SNE of projector outputs and word embeddings.\n\nParameters\n----------\nprojections : torch.Tensor\n    Output of the projector with shape ``(N, E)``.\ndataset : HTRDataset\n    Provides ``external_word_embeddings`` of shape ``(V, E)``.\nsave_path : str\n    Destination path for the PNG figure."
    },
    {
      "id": "alignment.plot.plot_pretrained_backbone_tsne",
      "type": "function",
      "doc": "Plot t-SNE embeddings from the pretrained backbone.\n\nParameters\n----------\ndataset : HTRDataset\n    Dataset instance providing images and alignment labels.\nn_samples : int\n    Number of random samples to visualise.\nsave_path : str\n    Path where the PNG figure will be saved."
    },
    {
      "id": "alignment.alignment_trainer",
      "type": "module",
      "doc": ""
    },
    {
      "id": "alignment.alignment_trainer._assert_finite",
      "type": "function",
      "doc": ""
    },
    {
      "id": "alignment.alignment_trainer._assert_grad_finite",
      "type": "function",
      "doc": ""
    },
    {
      "id": "alignment.alignment_trainer.maybe_load_backbone",
      "type": "function",
      "doc": "Load pretrained backbone weights if ``cfg.load_pretrained_backbone``."
    },
    {
      "id": "alignment.alignment_trainer.refine_visual_backbone",
      "type": "function",
      "doc": "Fine\u2011tune *backbone* only on words already aligned to external words."
    },
    {
      "id": "alignment.alignment_trainer.train_projector",
      "type": "function",
      "doc": "Freeze `backbone`, collect all image descriptors, and then train the\n`projector` (or a list of projectors) using a combination of an\nunsupervised Optimal Transport loss on all samples and a supervised MSE\nloss on the subset of pre-aligned samples.\n\nAll images are first forwarded through the frozen `backbone` **without\naugmentation** to obtain a stable descriptor for every sample. These descriptors,\nalong with their alignment information, are cached in a temporary TensorDataset.\nThe `projector` is then optimised using this dataset."
    },
    {
      "id": "alignment.alignment_trainer.alternating_refinement",
      "type": "function",
      "doc": "Alternately train ``backbone`` and one or more projectors with OT alignment."
    },
    {
      "id": "alignment.alignment_utilities",
      "type": "module",
      "doc": "Utility functions for aligning dataset instances to external words."
    },
    {
      "id": "alignment.alignment_utilities.OTAligner",
      "type": "class",
      "doc": "Helper class implementing the OT pseudo-labelling routine."
    },
    {
      "id": "alignment.alignment_utilities.harvest_backbone_features",
      "type": "function",
      "doc": "Return image descriptors and alignment info for the whole dataset.\n\nDataset augmentations are temporarily disabled while collecting\ndescriptors. The backbone is run in evaluation mode on ``device``.\n\nParameters\n----------\ndataset : HTRDataset\n    Dataset providing images and alignment information.\nbackbone : HTRNet\n    Visual encoder used to extract per-image descriptors.\nbatch_size : int, optional\n    Mini-batch size when forwarding the dataset.\nnum_workers : int, optional\n    Worker processes used by the ``DataLoader``.\ndevice : torch.device | str, optional\n    Device on which the backbone runs.\n\nReturns\n-------\ntorch.Tensor\n    Tensor of descriptors with shape ``(N, D)`` where ``N`` is the\n    dataset size.\ntorch.Tensor\n    Alignment tensor of shape ``(N,)`` copied from the dataset."
    },
    {
      "id": "alignment.alignment_utilities.calculate_ot_projections",
      "type": "function",
      "doc": "Compute OT projections of ``X`` onto ``Y``.\n\nParameters\n----------\npa : np.ndarray\n    Source distribution over ``X`` of shape ``(N,)``.\nX : np.ndarray\n    Source features of shape ``(N, D)``.\npb : np.ndarray\n    Target distribution over ``Y`` of shape ``(M,)``.\nY : np.ndarray\n    Target features of shape ``(M, D)``.\nreg : float, optional\n    Entropic regularisation.\nunbalanced : bool, optional\n    Use unbalanced OT formulation if ``True``.\nreg_m : float, optional\n    Unbalanced mass regularisation.\nsinkhorn_kwargs : dict, optional\n    Additional arguments for the OT solver.\n\nReturns\n-------\nprojections : np.ndarray\n    ``X`` projected in the space of ``Y`` (``(N, D)``).\nplan : np.ndarray\n    Optimal transport plan of shape ``(N, M)``."
    },
    {
      "id": "alignment.alignment_utilities.select_uncertain_instances",
      "type": "function",
      "doc": "Return indices of the ``m`` most uncertain instances.\n\nParameters\n----------\nm : int\n    Number of indices to return.\ntransport_plan : np.ndarray, optional\n    OT plan of shape ``(N, V)``. Required for ``metric='entropy'``.\ndist_matrix : np.ndarray, optional\n    Pre-computed pairwise distances ``(N, V)``. Required for\n    ``metric='gap'``.\nmetric : str, optional\n    Either ``'gap'`` or ``'entropy'`` selecting the uncertainty measure.\n\nReturns\n-------\nnp.ndarray\n    Array of ``m`` indices sorted by decreasing uncertainty."
    },
    {
      "id": "alignment.alignment_utilities.align_more_instances",
      "type": "function",
      "doc": "Wrapper over :class:`OTAligner` for backward compatibility."
    },
    {
      "id": "alignment.ctc_utils",
      "type": "module",
      "doc": ""
    },
    {
      "id": "alignment.ctc_utils.encode_for_ctc",
      "type": "function",
      "doc": "Convert a batch of raw string transcriptions to the (targets, lengths)\nformat expected by ``nn.CTCLoss``.\n\nParameters\n----------\ntranscriptions : list[str]\n    Each element is a single line/word **already wrapped with leading\n    and trailing spaces** (the dataset does that for you).\nc2i : dict[str, int]\n    Character-to-index mapping where **index 0 is reserved for CTC blank**\n    and every real character starts at 1.\ndevice : torch.device, optional\n    If given, the returned tensors are moved to this device.\n\nReturns\n-------\ntargets : torch.IntTensor   # shape = (total_chars,)\n    All label indices concatenated in batch order.\nlengths : torch.IntTensor   # shape = (batch,)\n    The original length (in characters) of every element in *transcriptions*."
    },
    {
      "id": "alignment.ctc_utils.greedy_ctc_decode",
      "type": "function",
      "doc": "Greedy-decode a batch of CTC network outputs.\n\nParameters\n----------\nlogits : torch.Tensor\n    Tensor shaped either (T, B, C) if *time_first* is True\n    **or** (B, T, C) if *time_first* is False, where  \n    *T = time steps*, *B = batch size*, *C = n_classes*.\n    It can contain raw scores, logits, or probabilities \u2013\n    only the arg-max along *C* is used.\ni2c : dict[int, str]\n    Index-to-character mapping that complements the *c2i*\n    used during encoding. It must **not** contain the blank id.\nblank_id : int, optional\n    Integer assigned to the CTC blank (defaults to 0).\ntime_first : bool, optional\n    Set **True** if logits are (T, B, C); otherwise set False\n    for (B, T, C).\n\nReturns\n-------\nList[str]\n    One decoded string per element in the mini-batch."
    },
    {
      "id": "alignment.ctc_utils.beam_search_ctc_decode",
      "type": "function",
      "doc": "Beam-search decoding for CTC outputs using pyctcdecode.\n\nParameters\n----------\nlogits : torch.Tensor\n    Network output \u2013 either ``(T, B, C)`` if *time_first* or ``(B, T, C)``.\ni2c : dict[int, str]\n    Index-to-character map **excluding** the blank id.\nbeam_width : int, optional\n    Number of prefixes kept after every time-step.\nblank_id : int, optional\n    Integer assigned to the CTC blank (defaults to ``0``).\ntime_first : bool, optional\n    ``True`` if *logits* are ``(T, B, C)``, else ``False`` for ``(B, T, C)``.\n\nReturns\n-------\nlist[str]\n    Best-scoring transcription for every element in the mini-batch."
    },
    {
      "id": "alignment.eval",
      "type": "module",
      "doc": "Evaluation helpers for alignment module."
    },
    {
      "id": "alignment.eval._assert_finite",
      "type": "function",
      "doc": ""
    },
    {
      "id": "alignment.eval.compute_cer",
      "type": "function",
      "doc": "Return character error rate on *dataset* using *model*.\n\nParameters\n----------\ndataset : Dataset\n    Items yield ``(img, transcription, _)`` triples as in ``HTRDataset``.\nmodel : torch.nn.Module\n    Network returning CTC logits (``(T,B,C)``).\nbatch_size : int, optional\n    Mini-batch size used during evaluation.\ndevice : str or torch.device, optional\n    Compute device for the forward pass.\ndecode : {'greedy', 'beam'}, optional\n    Decoding strategy for CTC outputs.\nbeam_width : int, optional\n    Beam width when ``decode='beam'``.\nk : int, optional\n    If given, also report CER for samples of length ``<= k`` and ``> k``.\n\nReturns\n-------\nfloat\n    Overall CER over the dataset."
    },
    {
      "id": "htr_base.prepare_gw",
      "type": "module",
      "doc": "Usage example:\npython prepare_gw.py     --dataset_folder /path/to/GW     --output_path ./data/GW/processed_word     --level word\n\nOr for line-level processing:\npython prepare_gw.py     --dataset_folder /path/to/GW     --output_path ./data/GW/processed_line     --level line"
    },
    {
      "id": "htr_base.prepare_gw.decide_split",
      "type": "function",
      "doc": ""
    },
    {
      "id": "htr_base.prepare_gw.load_word_data",
      "type": "function",
      "doc": ""
    },
    {
      "id": "htr_base.prepare_gw.load_line_data",
      "type": "function",
      "doc": ""
    },
    {
      "id": "htr_base.prepare_gw.prepare",
      "type": "function",
      "doc": ""
    },
    {
      "id": "htr_base.__init__",
      "type": "module",
      "doc": "Core HTR base package."
    },
    {
      "id": "htr_base.models",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.models.Projector",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.BasicBlock",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.CNN",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.AttentivePool",
      "type": "class",
      "doc": "Collapses a feature map via learnable attention weights."
    },
    {
      "id": "htr_base.models.CTCtopC",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.CTCtopR",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.CTCtopB",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.models.CTCtopT",
      "type": "class",
      "doc": "Transformer-based CTC head."
    },
    {
      "id": "htr_base.models.HTRNet",
      "type": "class",
      "doc": "HTRNet backbone with optional global feature projection.\n\nAdditional arg in *arch_cfg*:\n    feat_dim (int | None): size of per-image descriptor."
    },
    {
      "id": "htr_base.utils.vocab",
      "type": "module",
      "doc": "Utility helpers for the fixed character vocabulary."
    },
    {
      "id": "htr_base.utils.vocab.create_vocab",
      "type": "function",
      "doc": "Create the default vocabulary pickles and return the dictionaries.\n\nThe mapping contains digits ``0``\u2013``9``, lowercase ``a``\u2013``z`` and a\nspace character.  Index ``0`` is reserved for the CTC blank."
    },
    {
      "id": "htr_base.utils.vocab.load_vocab",
      "type": "function",
      "doc": "Load ``c2i`` and ``i2c`` dictionaries from ``saved_models``.\n\nIf the pickle files do not exist, :func:`create_vocab` is called to\ngenerate them first."
    },
    {
      "id": "htr_base.utils.htr_dataset",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.htr_dataset.HTRDataset",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.utils.htr_dataset.PretrainingHTRDataset",
      "type": "class",
      "doc": "Lightweight dataset for image-only pretraining.\n\nIf ``n_random`` is provided, ``random_seed`` ensures the same\nsubset of images is selected each time.  When ``preload_images``\nis ``True`` the raw images are cached in memory for faster\naccess."
    },
    {
      "id": "htr_base.utils.preprocessing",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.preprocessing.load_image",
      "type": "function",
      "doc": "Read *path* and return a float32 array in [0,1] whose background is black\nand foreground (text) is white.\n\nParameters\n----------\nensure_black_bg : bool, default True\n    If False, keep the legacy unconditional inversion.\nbg_thresh : float, default 0.55\n    When `ensure_black_bg` is True we compute the **median** gray value of\n    the image (after rgb\u2192gray, before any inversion).  If the median is\n    *brighter* than `bg_thresh` we invert, otherwise we keep the pixel\n    values as-is."
    },
    {
      "id": "htr_base.utils.preprocessing.preprocess",
      "type": "function",
      "doc": "Resize ``img`` and symmetrically pad it to ``input_size``."
    },
    {
      "id": "htr_base.utils.metrics",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.metrics.CER",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.utils.metrics.WER",
      "type": "class",
      "doc": ""
    },
    {
      "id": "htr_base.utils.metrics.word_silhouette_score",
      "type": "function",
      "doc": "Return the average silhouette coefficient for *features* labelled by *words*.\n\nParameters\n----------\nfeatures : torch.Tensor of shape ``(N, D)``\n    Backbone descriptors.\nwords : list[str]\n    Ground-truth word for each descriptor.\n\nReturns\n-------\nfloat\n    Mean silhouette score in ``[-1, 1]``."
    },
    {
      "id": "htr_base.utils.__init__",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.transforms",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.phoc",
      "type": "module",
      "doc": ""
    },
    {
      "id": "htr_base.utils.phoc.build_phoc_description",
      "type": "function",
      "doc": "Convert *words* into binary PHOC descriptors.\n\nParameters\n----------\nwords : list[str]\n    Words to encode.\nc2i : dict[str, int]\n    Mapping from character to index. This dictionary **does not**\n    contain the CTC blank which is implicitly reserved at index 0.\nlevels : list[int], optional\n    Pyramid levels. Default ``(1, 2, 3, 4)``.\n\nReturns\n-------\ntorch.BoolTensor\n    Tensor of shape ``(B, len(c2i) * sum(levels))`` with PHOC descriptors."
    }
  ],
  "edges": [
    {
      "source": "alignment.pretraining",
      "target": "htr_base.utils.htr_dataset",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "htr_base.models",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "htr_base.utils.transforms",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "htr_base.utils.metrics",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "htr_base.utils.vocab",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "alignment.ctc_utils",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "alignment.losses",
      "type": "import"
    },
    {
      "source": "alignment.pretraining",
      "target": "htr_base.utils.phoc",
      "type": "import"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "htr_base.utils.htr_dataset.PretrainingHTRDataset",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "htr_base.utils.vocab.load_vocab",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "htr_base.utils.metrics.CER",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.losses.SoftContrastiveLoss",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.ctc_utils.greedy_ctc_decode",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.ctc_utils.beam_search_ctc_decode",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.ctc_utils.encode_for_ctc",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.losses._ctc_loss_fn",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.eval._assert_finite",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "alignment.pretraining._check_grad_finite",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "htr_base.models.HTRNet",
      "type": "call"
    },
    {
      "source": "alignment.pretraining.main",
      "target": "htr_base.utils.phoc.build_phoc_description",
      "type": "call"
    },
    {
      "source": "alignment.plot",
      "target": "htr_base.utils.htr_dataset",
      "type": "import"
    },
    {
      "source": "alignment.plot",
      "target": "htr_base.models",
      "type": "import"
    },
    {
      "source": "alignment.plot",
      "target": "htr_base.utils.vocab",
      "type": "import"
    },
    {
      "source": "alignment.plot",
      "target": "alignment.alignment_utilities",
      "type": "import"
    },
    {
      "source": "alignment.plot.plot_tsne_embeddings",
      "target": "alignment.alignment_utilities.harvest_backbone_features",
      "type": "call"
    },
    {
      "source": "alignment.plot.plot_pretrained_backbone_tsne",
      "target": "htr_base.utils.vocab.load_vocab",
      "type": "call"
    },
    {
      "source": "alignment.plot.plot_pretrained_backbone_tsne",
      "target": "htr_base.models.HTRNet",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "htr_base.utils.htr_dataset",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "htr_base.models",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "alignment.losses",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "alignment.ctc_utils",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "alignment.alignment_utilities",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "alignment.eval",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "alignment.plot",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "htr_base.utils.metrics",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "htr_base.utils.transforms",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer",
      "target": "htr_base.utils.vocab",
      "type": "import"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "htr_base.utils.vocab.load_vocab",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "htr_base.utils.metrics.word_silhouette_score",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "alignment.eval._assert_finite",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "alignment.ctc_utils.encode_for_ctc",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "alignment.losses._ctc_loss_fn",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "alignment.alignment_trainer._assert_grad_finite",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "alignment.alignment_utilities.harvest_backbone_features",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.refine_visual_backbone",
      "target": "htr_base.utils.phoc.build_phoc_description",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.train_projector",
      "target": "alignment.alignment_utilities.harvest_backbone_features",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.train_projector",
      "target": "alignment.plot.plot_tsne_embeddings",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.train_projector",
      "target": "alignment.losses.ProjectionLoss",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.train_projector",
      "target": "alignment.plot.plot_projector_tsne",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.alternating_refinement",
      "target": "alignment.alignment_trainer.maybe_load_backbone",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.alternating_refinement",
      "target": "alignment.alignment_utilities.align_more_instances",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.alternating_refinement",
      "target": "alignment.eval.compute_cer",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.alternating_refinement",
      "target": "alignment.alignment_trainer.refine_visual_backbone",
      "type": "call"
    },
    {
      "source": "alignment.alignment_trainer.alternating_refinement",
      "target": "alignment.alignment_trainer.train_projector",
      "type": "call"
    },
    {
      "source": "alignment.alignment_utilities",
      "target": "htr_base.utils.htr_dataset",
      "type": "import"
    },
    {
      "source": "alignment.alignment_utilities",
      "target": "htr_base.models",
      "type": "import"
    },
    {
      "source": "alignment.alignment_utilities.OTAligner",
      "target": "alignment.alignment_utilities.calculate_ot_projections",
      "type": "call"
    },
    {
      "source": "alignment.alignment_utilities.OTAligner",
      "target": "alignment.alignment_utilities.harvest_backbone_features",
      "type": "call"
    },
    {
      "source": "alignment.alignment_utilities.OTAligner",
      "target": "alignment.alignment_utilities.select_uncertain_instances",
      "type": "call"
    },
    {
      "source": "alignment.alignment_utilities.align_more_instances",
      "target": "alignment.alignment_utilities.OTAligner",
      "type": "call"
    },
    {
      "source": "alignment.eval",
      "target": "htr_base.utils.metrics",
      "type": "import"
    },
    {
      "source": "alignment.eval",
      "target": "htr_base.utils.vocab",
      "type": "import"
    },
    {
      "source": "alignment.eval.compute_cer",
      "target": "htr_base.utils.vocab.load_vocab",
      "type": "call"
    },
    {
      "source": "alignment.eval.compute_cer",
      "target": "htr_base.utils.metrics.CER",
      "type": "call"
    },
    {
      "source": "alignment.eval.compute_cer",
      "target": "alignment.eval._assert_finite",
      "type": "call"
    },
    {
      "source": "alignment.eval.compute_cer",
      "target": "alignment.ctc_utils.beam_search_ctc_decode",
      "type": "call"
    },
    {
      "source": "alignment.eval.compute_cer",
      "target": "alignment.ctc_utils.greedy_ctc_decode",
      "type": "call"
    },
    {
      "source": "htr_base.prepare_gw.load_word_data",
      "target": "htr_base.prepare_gw.decide_split",
      "type": "call"
    },
    {
      "source": "htr_base.prepare_gw.load_line_data",
      "target": "htr_base.prepare_gw.decide_split",
      "type": "call"
    },
    {
      "source": "htr_base.models.CNN",
      "target": "htr_base.models.BasicBlock",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.CNN",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.CTCtopC",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.CTCtopR",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.AttentivePool",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.CTCtopB",
      "type": "call"
    },
    {
      "source": "htr_base.models.HTRNet",
      "target": "htr_base.models.CTCtopT",
      "type": "call"
    },
    {
      "source": "htr_base.utils.vocab.load_vocab",
      "target": "htr_base.utils.vocab.create_vocab",
      "type": "call"
    },
    {
      "source": "htr_base.utils.htr_dataset",
      "target": "htr_base.utils.vocab",
      "type": "import"
    },
    {
      "source": "htr_base.utils.htr_dataset.HTRDataset",
      "target": "htr_base.utils.preprocessing.load_image",
      "type": "call"
    },
    {
      "source": "htr_base.utils.htr_dataset.HTRDataset",
      "target": "htr_base.utils.preprocessing.preprocess",
      "type": "call"
    },
    {
      "source": "htr_base.utils.htr_dataset.HTRDataset",
      "target": "htr_base.utils.vocab.load_vocab",
      "type": "call"
    },
    {
      "source": "htr_base.utils.htr_dataset.PretrainingHTRDataset",
      "target": "htr_base.utils.preprocessing.preprocess",
      "type": "call"
    },
    {
      "source": "htr_base.utils.htr_dataset.PretrainingHTRDataset",
      "target": "htr_base.utils.preprocessing.load_image",
      "type": "call"
    }
  ]
}